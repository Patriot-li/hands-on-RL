{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39803b05-ddd9-4c3b-8c7e-e0de80266373",
   "metadata": {},
   "source": [
    "# 2马尔可夫过程 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719efecc-6d48-4e1b-bbd9-1cb09644958f",
   "metadata": {},
   "source": [
    "当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质（Markov property），用公式表示为： \n",
    "$$\n",
    "\\begin{align}\n",
    "P(St+1\\mid St)=P(St+1\\mid S1,\\cdots,St)\n",
    "\\end{align}\n",
    "$$  \n",
    "具有马尔可夫性质的随机过程被称为马尔可夫过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9547b49-3673-4f6a-954b-a4d9fd15da21",
   "metadata": {},
   "source": [
    "## 2.1马尔可夫奖励过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1911e-9b8d-4a1b-a190-ede7a729503f",
   "metadata": {},
   "source": [
    "一个epsiod的总回报计算如下：  \n",
    "$$\n",
    "\\begin{align}\n",
    "{{G}_{t}}={{R}_{t}}+\\gamma {{R}_{t+1}}+{{\\gamma}^{2}}{{R}_{t+2}}+\\cdots=\\sum\\limits_{k=0}^{\\infty }{{{\\gamma }^{k}}{{R}_{t+k}}}\n",
    "\\end{align}\n",
    "$$\n",
    "这里引入一个折扣因子来调节，折扣因子趋于1则更关注长期的累计奖励，折扣因子趋于0则更关注短期奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c358234c-0746-47b1-9aab-7e0bce9ac4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3abbdf9c-a84f-472a-8793-9b108fe25c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c65311-0b90-4215-b0cb-abd7d18d4464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据本序列计算得到回报为：-2.5。\n"
     ]
    }
   ],
   "source": [
    "S = [1, 2, 3, 4, 5, 6] #定义状态集合S（有限集）\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    "P = np.array(P) #定义状态转移矩阵P\n",
    "gamma = 0.5\n",
    "\n",
    "def reward_fn(state): #定义奖励函数（返回每个状态的奖励值）\n",
    "    Rewards = [-1, -2, -2, 10, 1, 0] #每个状态的奖励值\n",
    "    r = Rewards[state - 1]\n",
    "    return r\n",
    "\n",
    "def total_reward(epsiod):\n",
    "    G = 0\n",
    "    for state in reversed(epsiod): #通过反转实现折扣因子的幂次增加\n",
    "        G = gamma * G + reward_fn(state)\n",
    "    return G\n",
    "\n",
    "epsiod = [1, 2, 3, 6]\n",
    "G = total_reward(epsiod)\n",
    "print(f'根据本序列计算得到回报为：{G}。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626644d-9f5a-4ca2-9aed-e11cfbc5c250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654984f4-2b58-4fb7-9436-1259718c9558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_gym",
   "language": "python",
   "name": "rl_gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
