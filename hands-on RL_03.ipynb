{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96cd42f8-b94a-4e7b-a2ae-5a3bc8ed84d9",
   "metadata": {},
   "source": [
    "# 3动态规划算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1887fa6-fab5-4962-b61a-e2e4a4c73913",
   "metadata": {},
   "source": [
    "定义本章所用环境:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e80069-7643-4741-bcbd-90d3dd9b2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f0af4b-cafa-4498-9014-723cde4bb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 2, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env.render()\n",
    "a = env.observation_space\n",
    "print(env.P[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7860da-aaf7-4b38-8a37-64199827f252",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1Value迭代算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f10c9-652b-45e0-bc68-6d8d1c2edef1",
   "metadata": {},
   "source": [
    "策略迭代算法主要由两个模块组成： \n",
    "- 通过迭代计算optimal value function.  \n",
    "- 根据得到的value function 提取策略（policy）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf3bc9-47a7-4e67-87b1-2894f607a0ee",
   "metadata": {},
   "source": [
    "#### 3.1.1 computing the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "230c5300-0bb9-4150-b908-c80db47e9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, num_iter):\n",
    "    threshold = 1e-10\n",
    "    gamma = 1\n",
    "    #初始化value function\n",
    "    value_table = np.zeros(env.observation_space.n) #该问题中value function是一个table\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        old_value_table = np.copy(value_table) #为了保留上一步的value_table 我们要新建一个table来进行迭代更新\n",
    "        for s in range(env.observation_space.n):\n",
    "            Q_values = [] #记录s状态下的所有action的Q（s，a）\n",
    "            for a in range(env.action_space.n):\n",
    "                q_values = 0\n",
    "                for prob, s_, r, done in env.P[s][a]:\n",
    "                    q_values += prob * (r + gamma * value_table[s_])\n",
    "                Q_values.append(q_values)\n",
    "            value_table[s] = max(Q_values)  #Q与V的关系\n",
    "        if sum(np.fabs(value_table - old_value_table)) <= threshold:\n",
    "            break\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8581a-a971-4a31-b88d-e9d3b722b354",
   "metadata": {},
   "source": [
    "#### 3.1.2 extract the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331412fe-5fa4-4ef4-8d38-4f35b65a3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(value_table):\n",
    "    gamma = 1.0\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        Q_values = [] #记录s状态下的所有action的Q（s，a）\n",
    "        for a in range(env.action_space.n):\n",
    "            q_values = 0\n",
    "            for prob, s_, r, done in env.P[s][a]:\n",
    "                q_values += prob * (r + gamma * value_table[s_])\n",
    "            Q_values.append(q_values)\n",
    "        policy[s] = np.argmax(np.array(Q_values))\n",
    "    return policy        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0daaab15-60d6-471f-af7d-b06aca8f6260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function is:\n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "policy is:\n",
      "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(env, 1000)\n",
    "policy = extract_policy(V)\n",
    "print('value function is:', V, sep='\\n')\n",
    "print('policy is:', policy, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88a583-e370-4e62-927c-4ff592f233b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2Policy迭代算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ec8ec-cb72-4196-bcb1-af88167f5909",
   "metadata": {},
   "source": [
    "policy迭代算法主要由以下四步组成:  \n",
    "- 初始化policy  \n",
    "- 使用所给的policy计算value function  \n",
    "- 使用所得的value function计算新的policy  \n",
    "- 判断现有policy是否为optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eed2b69-1eba-4f64-8bd3-d1faab4c998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''计算value function'''\n",
    "def compute_value_fn(policy):\n",
    "    num_iter = 1000\n",
    "    threshold = 1e-20\n",
    "    gamma = 1\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        old_value_table = np.copy(value_table)\n",
    "        for s in range(env.observation_space.n):\n",
    "            a = policy[s]\n",
    "            value_table[s] = sum([prob * (r + gamma * value_table[s_]) for prob, s_, r, _ in env.P[s][a]])\n",
    "        if (np.sum(np.fabs(old_value_table - value_table)) <= threshold):\n",
    "            break\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b52781a5-b207-493a-9e2d-b48f44c4191a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'提取policy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''提取policy'''\n",
    "#此部分代码与上面extract policy相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56645dfc-25c3-4a39-a6f1-1606736df3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iter(env):\n",
    "    num_iter = 1000\n",
    "    #初始化policy\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        #计算该policy下的value function\n",
    "        value_function = compute_value_fn(policy)\n",
    "        #根据得到的value function提取policy\n",
    "        new_policy = extract_policy(value_function)\n",
    "        #判断是否为optimal policy\n",
    "        if (np.all(policy == new_policy)):\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7fe4e2a-e44e-4b4c-98ba-c2e36589a962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = policy_iter(env)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966163ff-366a-4de0-8d8b-557cbd33e58f",
   "metadata": {},
   "source": [
    "#### 在计算状态值函数的时候，我们是采用迭代计算的方法逐步近似到真实值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005838f5-fec7-43d8-8e3a-b956dd92b24b",
   "metadata": {},
   "source": [
    "#### 通过上述算法可知，我们要想计算V和Q function 就必须知道状态转移矩阵，也就是说环境的动态变化必须是已知的，所以DP算法也是model-based算法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_gym",
   "language": "python",
   "name": "rl_gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
