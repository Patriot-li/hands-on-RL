{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b7becb-6aa3-45a2-8a1d-5ef53048a7a1",
   "metadata": {},
   "source": [
    "# TD Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979fe2c9-37b1-43c9-94d7-f15ca71e38f6",
   "metadata": {},
   "source": [
    "### TD Prediction  \n",
    "TD与MC的却别在于更新value function时，MC是在每一个episode接收后进行更新，而TD是在每一个时间步之后更新，不必等到游戏结束.  \n",
    "$$\n",
    "\\begin{align}\n",
    "V(s_t)\\leftarrow V(s_t)+\\alpha[r_t+\\gamma V(s_{t+1})-V(s_t)]\n",
    "\\end{align}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa6766c-049f-4cb2-8430-fb376caf363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义本章所用环境\n",
    "import gym\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1452f49a-d4f2-4150-8895-14096277c7d0",
   "metadata": {},
   "source": [
    "### TD Prediction algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf007c42-139c-4d11-844d-0a0c8217089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个随机policy\n",
    "def policy():\n",
    "    return env.action_space.sample()\n",
    "\n",
    "#初始化value function\n",
    "V = {}\n",
    "for s in range(env.observation_space.n):\n",
    "    V[s] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a278bb8c-db91-4091-9447-927d58d6199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.85 #学习率\n",
    "gamma = 0.9 #折减系数\n",
    "\n",
    "num_episode = 1000\n",
    "num_step = 100\n",
    "\n",
    "#计算value function\n",
    "for i in range(num_episode):\n",
    "    s = env.reset()\n",
    "    for t in range(num_step): #每一步都进行value function的更新\n",
    "        a = policy()\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        V[s] += alpha * (r + gamma * V[s_] - V[s])\n",
    "        if done:\n",
    "            break\n",
    "        s = s_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c517b7d-4ed3-4f81-acf9-c8166c30b7cd",
   "metadata": {},
   "source": [
    "通过上述算法可以发现，在episode每一个时间步中，V都会进行迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de3422-deee-4180-9d7b-72d3fb8deaaf",
   "metadata": {},
   "source": [
    "### TD Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90ff94-c31d-43ad-b95e-fb0390ce0bca",
   "metadata": {},
   "source": [
    "- SARSA —— On-polivy TD control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5b0bd-9c1b-44b0-ac69-1a4aefb98707",
   "metadata": {},
   "source": [
    "从前面的学习可以知道，我们从Q function中提取policy，求解optimal policy实际上就是在求解Q function.所以只需将V function换成更新Q function 就是 SARSA算法。  \n",
    "$$\n",
    "\\begin{align}\n",
    "Q(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha[r_t+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]\n",
    "\\end{align}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "514a7ec8-549d-4065-abc0-ce01b1939152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化Q function\n",
    "Q = {}\n",
    "for s in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        Q[(s, a)] = 0.0\n",
    "\n",
    "#定义policy\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)), key = lambda x : Q[(s, x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2efe1c17-7f29-448c-9cd8-3cc364456b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "gamma = 0.9\n",
    "epsilon = 0.8\n",
    "\n",
    "num_episode = 1000\n",
    "num_step = 100\n",
    "\n",
    "#计算Q function ，即提取optimal policy\n",
    "for i in range(num_episode):\n",
    "    s = env.reset()\n",
    "    a = epsilon_greedy_policy(s, epsilon)\n",
    "    for t in range(num_step):\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        a_ = epsilon_greedy_policy(s_, epsilon)\n",
    "        Q[(s, a)] += alpha * (r + gamma * Q[(s_, a_)] - Q[(s, a)])\n",
    "        if done:\n",
    "            break\n",
    "        s = s_\n",
    "        a = a_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc305b3-b166-4985-b94a-159a89068c24",
   "metadata": {},
   "source": [
    "- Q learning —— off-policy TD control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd08b0-349d-4937-aed6-dbf5486fa228",
   "metadata": {},
   "source": [
    "其与SARSA的区别在于，在算法中：与环境交互的policy和计算Q时用的policy不是同一个policy。  \n",
    "在这里，与环境交互用的是epsilon-greedy policy；计算Q时用的是greedy-policy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc43b96-9e5f-4ba3-b1c2-6a55cbc1a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化Q\n",
    "Q = {}\n",
    "for s in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        Q[(s, a)] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec12667c-2047-4531-afdd-87d3ea0ab03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义policy\n",
    "\n",
    "#epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)), key = lambda x : Q[(state, x)])\n",
    "    \n",
    "#greedy policy\n",
    "def greedy_policy(state):\n",
    "    return np.argmax([Q[(s_, a)] for a in range(env.action_space.n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "651ff37f-6e1c-4f11-a929-af2ac8ba9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取optimal Q function\n",
    "alpha = 0.85\n",
    "gamma = 0.90\n",
    "epsilon = 0.8\n",
    "\n",
    "num_episode = 1000\n",
    "num_step = 100\n",
    "\n",
    "for i in range(num_episode):\n",
    "    s = env.reset()\n",
    "    for t in range(num_step):\n",
    "        a = epsilon_greedy_policy(s, epsilon) #与环境交互的policy\n",
    "        s_ , r, done, _ = env.step(a)\n",
    "        a_ = greedy_policy(s_) #用于更新Q的policy\n",
    "        Q[(s, a)] += alpha * (r + gamma * Q[(s_, a_)] - Q[(s, a)])\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b926112-2a71-459a-8909-903d9a839f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state-action</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>0.210561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>0.237352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>0.301010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>0.334825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1, 0)</td>\n",
       "      <td>0.035253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.181317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.000964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>0.168928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(2, 0)</td>\n",
       "      <td>0.320543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(2, 1)</td>\n",
       "      <td>0.297513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>0.395731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state-action     value\n",
       "0        (0, 0)  0.210561\n",
       "1        (0, 1)  0.237352\n",
       "2        (0, 2)  0.301010\n",
       "3        (0, 3)  0.334825\n",
       "4        (1, 0)  0.035253\n",
       "5        (1, 1)  0.181317\n",
       "6        (1, 2)  0.000964\n",
       "7        (1, 3)  0.168928\n",
       "8        (2, 0)  0.320543\n",
       "9        (2, 1)  0.297513\n",
       "10       (2, 2)  0.395731"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(Q.items(), columns = ['state-action', 'value'])\n",
    "df.head(11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_gym",
   "language": "python",
   "name": "rl_gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
